{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6750d916",
   "metadata": {},
   "source": [
    "# Lesson 2.5 — Circle Classification with PyTorch\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this lesson, we will learn the **core PyTorch training workflow** by solving a very small\n",
    "classification problem.\n",
    "\n",
    "Basically, go through: \n",
    "\n",
    "![pytorchio](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-training-loop-annotated.png)\n",
    "\n",
    "We will focus on:\n",
    "- building a neural network with `nn.Module`\n",
    "- defining a loss function\n",
    "- choosing an optimizer\n",
    "- writing a training loop\n",
    "- writing a testing (validation) loop\n",
    "\n",
    "We deliberately avoid:\n",
    "- DataLoaders\n",
    "- custom Dataset classes\n",
    "- CNNs\n",
    "- images\n",
    "\n",
    "This lets us focus on the *ideas* that matter. Everything you see here will reappear later\n",
    "when we train CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc55228-12b6-4fa5-8b6b-c9aa783a3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "981b268d-3e2a-4cea-b495-073478ffd9a8",
   "metadata": {},
   "source": [
    "## 1. Create a Nonlinear Classification Problem\n",
    "\n",
    "Here, we construct a simple but instructive classification problem using synthetic data. We generate 1000 data points, where each point has two input features: \n",
    "\n",
    "$x = (x_1, x_2)$\n",
    "   \n",
    "The points are arranged as two concentric circles, as shown in the visualization below, with a small amount of noise added to make the problem more realistic. We then split the dataset into:\n",
    "- Training data (80%), used to learn the model parameters.\n",
    "- Test data (20%), used to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "The visualization below shows only the training data, colored by classes:\n",
    "- Class 0 (inner circle) (blue color)\n",
    "- Class 1 (outer circle) (red color)\n",
    "\n",
    "This dataset is deliberately chosen because no straight line can separate the two classes. As a result, linear models such as linear regression or logistic regression will struggle. However, even a small neural network can learn a nonlinear decision boundary that separates the two circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487ef52-2cf4-4689-b9e8-87366cf6d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "X, y = make_circles(n_samples=1000,\n",
    "    noise=0.03,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdBu);\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcfa14",
   "metadata": {},
   "source": [
    "Choose CPU or GPU if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c274ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71402710-059c-4a96-9040-a0ff9a67a5ae",
   "metadata": {},
   "source": [
    "Initially was numpy array, as discussed in intro to pytorch, convert to torch tensors + move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53168730-248e-4513-9078-e22a6bb1fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors (train/test)\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32, device=device)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long, device=device)\n",
    "\n",
    "X_train_t.shape, y_train_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad3a6d-f3cb-44c8-9e08-c3fbef394e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to be sure \n",
    "print(X_train_t.device, y_train_t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6599703",
   "metadata": {},
   "source": [
    "## 2. Define a simple neural network\n",
    "\n",
    "Here, we define a simple neural network that learns to decide which class or circle a point belongs to. The network works in three steps:\n",
    "\n",
    "- **Hidden Layer**: The network examines the input point and generates 8 new values from it. You do not need to worry yet about what these values mean,  they are just intermediate numbers that help the network learn patterns.\n",
    "\n",
    "- **ReLU activation**: This step adds non-linearity, which allows the network to learn curved boundaries instead of straight lines. This is the key reason neural networks can solve this problem.\n",
    "\n",
    "- **Output**: The network produces two numbers (logits). These two numbers are raw scores that the model uses to decide between classes, one for each class: inner circle and outer circle. The model predicts the class corresponding to the larger of these two numbers. Suppose the model outputs logits for a point: [ 2.3 , -1.1 ]\n",
    "\n",
    "  Score for class 0 = 2.3\n",
    "\n",
    "  Score for class 1 = −1.1\n",
    "\n",
    "  Since 2.3 is larger, the model predicts the point to be in class 0 (inner circle).\n",
    "\n",
    "This network is small, but it is powerful enough to separate the two circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85025cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd6cf6",
   "metadata": {},
   "source": [
    "Create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b7e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CircleNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c90df",
   "metadata": {},
   "source": [
    "## 3. Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c6ba2",
   "metadata": {},
   "source": [
    "- The **loss function** tells the model how wrong it is\n",
    "- The **optimizer** updates the model parameters using gradients\n",
    "\n",
    "## 4. Training loop\n",
    "\n",
    "This loop matches the standard PyTorch training pattern.\n",
    "\n",
    "An **epoch** means one complete pass through all of the training data. Here we have 800 training points, so in each epoch the model sees all 800 points once. We repeat this process for many epochs (500) so the model can gradually learn, adjusting the neural network's weights with each pass through the data. \n",
    "\n",
    "What happens with each epoch during the training loop?\n",
    "\n",
    "- Forward pass: The model takes the input data and produces raw predictions (called logits) \n",
    "- Loss computation: The loss function compares the predictions to the true labels and measures how wrong the model is.\n",
    "- Zeroing gradients: Gradients from the previous step are cleared so they do not accumulate across iterations.\n",
    "- Backward pass: PyTorch computes how the loss changes with respect to each model parameter using automatic differentiation.\n",
    "- Optimizer step: The optimizer updates the model parameters in the direction that reduces the loss.\n",
    "\n",
    "We compute train accuracy from the same logits we just used for the loss, but test accuracy needs a separate forward pass because we never train on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1c2e1-bd6c-45e6-94d9-22df4a1462ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "train_loss_history = []\n",
    "test_loss_history  = []\n",
    "train_acc_history  = []\n",
    "test_acc_history   = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # 1. Forward pass (TRAIN)\n",
    "    train_logits = model(X_train_t)  # logits = raw model outputs (one score per class)\n",
    "\n",
    "    # 2. Compute loss (TRAIN)\n",
    "    train_loss = loss_fn(train_logits, y_train_t)\n",
    "\n",
    "    # 3. Reset the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Backward pass\n",
    "    train_loss.backward()\n",
    "\n",
    "    # 5. Optimizer step (gradient descent update)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save TRAIN metrics\n",
    "    train_loss_history.append(train_loss.item())\n",
    "    train_acc = accuracy_from_logits(train_logits, y_train_t)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Evaluate on TEST data (no gradients)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 6. Forward pass (TEST)\n",
    "        test_logits = model(X_test_t)\n",
    "\n",
    "        # 7. Compute loss (TEST)\n",
    "        test_loss = loss_fn(test_logits, y_test_t)\n",
    "\n",
    "        # Save TEST metrics\n",
    "        test_loss_history.append(test_loss.item())\n",
    "        test_acc = accuracy_from_logits(test_logits, y_test_t)\n",
    "        test_acc_history.append(test_acc)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"Train loss: {train_loss.item():.4f} | Train acc: {train_acc:.3f} | \"\n",
    "            f\"Test loss: {test_loss.item():.4f} | Test acc: {test_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "print(\"Done training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7e3b3-f276-4ccd-a304-eb32d1000f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_loss_history, color='b', label='train')\n",
    "plt.plot(test_loss_history, color='r', label='test')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Over Time\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73fff8",
   "metadata": {},
   "source": [
    "Todo: discuss test/train loss functions -- what would loss function look like on test data if we had overfitting?\n",
    "\n",
    "The plot above shows training and test loss decreasing together, which indicates that the model is learning meaningful patterns and generalizing well to unseen data.\n",
    "\n",
    "If the model were overfitting, we would expect the training loss to continue decreasing while the test loss would stop improving and begin to increase, as shown in the illustrative plot below. This would indicate that the model is memorizing the training data rather than learning general patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78785ece-ec70-4270-9419-49a7c1c9c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrative example of overfitting (not from the model)\n",
    "epochs = len(train_loss_history)\n",
    "\n",
    "fake_train_loss = train_loss_history\n",
    "fake_test_loss = [\n",
    "    l if i < epochs // 3 else l + 0.002 * (i - epochs // 3)\n",
    "    for i, l in enumerate(train_loss_history)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(fake_train_loss, label=\"train (illustrative)\")\n",
    "plt.plot(fake_test_loss, label=\"test (illustrative overfitting)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Illustrative Example of Overfitting (not from the current model)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592885d8-ebd3-40a2-99b6-9ae574db0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(train_acc_history, label='train')\n",
    "plt.plot(test_acc_history, label='test')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Over Time\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56b5aa",
   "metadata": {},
   "source": [
    "The plot shows training accuracy and test accuracy as a function of training epochs.\n",
    "\n",
    "- At the beginning, both accuracies are close to 0.5, which is what we expect from random guessing in a two-class problem.\n",
    "\n",
    "- As training progresses, both curves rise rapidly.\n",
    "\n",
    "- After roughly 200 epochs, both training and test accuracy approach 1.0.\n",
    "\n",
    "The close alignment of training and test accuracy near 1.0 shows that the network has learned a correct and generalizable decision boundary for the circle classification problem.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Inspect predictions \n",
    "\n",
    "In this section, we examine a few randomly selected test samples to better understand how the trained model makes predictions. We randomly choose five points from the test set and compare:\n",
    "\n",
    "- The predicted class produced by the model\n",
    "\n",
    "- The true class label for each point\n",
    "\n",
    "This helps us verify that the model’s predictions align with the ground-truth labels on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6693bd-e7a9-4cca-982d-4a1e57694dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    test_logits = model(X_test_t)\n",
    "    predictions = torch.argmax(test_logits, dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(len(predictions))[:5]\n",
    "\n",
    "print(\"\\nRandom indices:\", idx.tolist())\n",
    "\n",
    "print(\"\\nPredicted classes:\")\n",
    "print(predictions[idx])\n",
    "\n",
    "print(\"\\nTrue classes:\")\n",
    "print(y_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622574c6-264c-4473-af92-710c78e8ec30",
   "metadata": {},
   "source": [
    "Inspecting individual predictions confirms model's high test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f776af-26f2-42bd-953c-4c23450863a2",
   "metadata": {},
   "source": [
    "## 6 Decision boundaries\n",
    "Let's look at learned decision boundaries. In the next cell, we visualize what the trained network has learned by evaluating it across a grid of possible input values. By asking the model to predict a class at every point in this input space, we can draw boundaries that show which regions lead to different output categories. \n",
    "\n",
    "Let's plot decision boundary and plot data for Test and Train with different marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d984192-058e-4e7d-a252-165d917f6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of points (CPU tensors are fine here; we'll move the stacked grid to GPU)\n",
    "x_min, x_max = -1.2, 1.2\n",
    "y_min, y_max = -1.2, 1.2\n",
    "\n",
    "xx, yy = torch.meshgrid(\n",
    "    torch.linspace(x_min, x_max, 200),\n",
    "    torch.linspace(y_min, y_max, 200),\n",
    "    indexing=\"ij\"\n",
    ")\n",
    "\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1).to(device)\n",
    "\n",
    "# Run model on grid (GPU)\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits = model(grid)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "Z = preds.reshape(xx.shape).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcefa35-c2d9-47f6-91db-db9067451041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision regions + data in subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)\n",
    "\n",
    "# Left: TRAIN\n",
    "axes[0].contourf(xx.numpy(), yy.numpy(), Z, cmap=\"coolwarm\", alpha=0.6)\n",
    "axes[0].scatter(\n",
    "    X_train[:, 0],\n",
    "    X_train[:, 1],\n",
    "    c=y_train,\n",
    "    cmap=\"coolwarm\",\n",
    "    edgecolors=\"k\",\n",
    "    s=30\n",
    ")\n",
    "axes[0].set_title(\"Train\")\n",
    "axes[0].set_xlabel(\"Input A\")\n",
    "axes[0].set_ylabel(\"Input B\")\n",
    "axes[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "axes[0].set_xlim(x_min, x_max)\n",
    "axes[0].set_ylim(y_min, y_max)\n",
    "\n",
    "# Right: TEST\n",
    "axes[1].contourf(xx.numpy(), yy.numpy(), Z, cmap=\"coolwarm\", alpha=0.6)\n",
    "axes[1].scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    c=y_test,\n",
    "    cmap=\"coolwarm\",\n",
    "    edgecolors=\"k\",\n",
    "    s=30\n",
    ")\n",
    "axes[1].set_title(\"Test\")\n",
    "axes[1].set_xlabel(\"Input A\")\n",
    "axes[1].set_ylabel(\"Input B\")\n",
    "axes[1].set_aspect(\"equal\", adjustable=\"box\")\n",
    "axes[1].set_xlim(x_min, x_max)\n",
    "axes[1].set_ylim(y_min, y_max)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ab7ff-8d33-492e-b9f3-5dabed2e6078",
   "metadata": {},
   "source": [
    "- The plots above show the learned decision boundary for both the training data (left) and the test data (right). The background color represents the class predicted by the model, while the points show the true data samples.\n",
    "\n",
    "- We observe that the same smooth decision boundary correctly separates the inner and outer circles in both cases. The model does not simply memorize the training data, instead, it learns a general rule that applies to unseen test points.\n",
    "\n",
    "This behavior indicates good generalization and a lack of overfitting. It is consistent with the training and test accuracy values, which are both close to 1.0, and with the similar training and test loss curves observed earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13905a68",
   "metadata": {},
   "source": [
    "## 7. Mapping to the PyTorch workflow\n",
    "\n",
    "This lesson covers **Steps 1–4** of the PyTorch workflow:\n",
    "\n",
    "1. Get data ready (tensors)\n",
    "2. Build a model\n",
    "3. Fit the model (training loop)\n",
    "4. Evaluate the model (testing loop)\n",
    "\n",
    "CNNs follow the **exact same structure** — they just use images instead of numbers.\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "If you understand this lesson, you understand the *core* of PyTorch training.\n",
    "\n",
    "Everything we do next (CNN inference, transfer learning, larger datasets) builds directly\n",
    "on this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68341cca-fecf-4952-b3df-ab217fa0e0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
