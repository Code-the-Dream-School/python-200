# Introduction to Data Pipelines

Imagine running your analysis once, then being asked to run it again tomorrow, next week, or on a larger dataset. Manually re-running each step - loading, cleaning, analyzing, and reporting - would be painful, error-prone, and time-consuming.

**Pipelines solve this**. They give us reproducible, modular workflows where each step is defined and orchestrated. This approach becomes especially powerful in **cloud computing**, a topic we‚Äôll explore in more detail in future lessons of Python 200.

### Learning objectives: 
By the end of this lesson, you‚Äôll be able to:

- See why pipelines make analysis reproducible, modular, and scalable.
- Build and orchestrate workflows using Prefect tasks and flows.
- Make pipelines resilient and efficient with retries and logging.
- Monitor and debug workflows in real time using the Orion dashboard.
- Run a full data analysis workflow - load, clean, explore, analyze, and report - with a single command.

### Table of Contents
1. Understanding Data Pipelines
2. Prefect
3. Building Your First Prefect Pipeline
4. Wrap-up

## 1. Understanding Data Pipelines

What is a **Data Pipeline**?

A **data pipeline** is a series of connected data processing steps where the output of one step becomes the input of the next. Think of it like a factory assembly line for data - raw materials (data) enter at one end, go through various transformation stations, and emerge as a finished product (insights, reports, or processed datasets).

```text
Raw Data ‚Üí Load ‚Üí Clean ‚Üí Analyze ‚Üí Visualize ‚Üí Report
```

If you‚Äôve ever written a script that loads data, cleans it, analyzes it, and saves results ‚Äî you‚Äôve already built a pipeline! You don‚Äôt need a special framework to have a pipeline.

Frameworks like **Prefect**, **Airflow**, or **Luigi** simply make it easier to manage, monitor, and automate complex workflows - especially when they run on large datasets, across multiple systems, or in the cloud.

The most important thing is the pipeline *mindset*: organizing your work into clear, ordered steps. Start with whatever tooling matches your project (a simple script is fine for exploration), and introduce orchestration tools only when you need scheduling, retries, observability, or cross-system coordination.

- Without pipelines ‚Üí manual re-runs, messy scripts, hard to reproduce results.
- With pipelines ‚Üí clear modular steps, easy to run end-to-end, scalable to cloud systems.

## Key Features & Benefits of Data Pipelines



Thinking in pipelines helps you turn ad-hoc code into reliable, repeatable workflows. By breaking an analysis into clear steps (extract ‚Üí transform ‚Üí analyze ‚Üí report), you make each part easier to test, document, and improve. Pipelines also let you **automate routine** work so you stop manually re-running scripts and reduce human error.

As projects grow, a pipeline mindset makes it natural to integrate **multiple data sources** and add operational features - scheduling, retries, logging, and monitoring - without rewriting your core analysis.

üí° In short: Start simple with a clear sequence of steps, and adopt more tooling only when you need the operational guarantees that frameworks provide.

## Types of Data Pipelines

Not all pipelines work the same way - their structure depends on how and when data moves. Here are three common styles:

### Batch Processing Pipelines
Batch pipelines process data in groups at scheduled intervals, such as hourly, nightly, or weekly. They‚Äôre ideal for large datasets that don‚Äôt require real-time processing. For example, generating daily sales reports or refreshing dashboards overnight.

### Streaming Data Pipelines
Streaming pipelines handle continuous data flow in real time. They‚Äôre useful for monitoring live or fast-moving sources, such as social media feeds, IoT sensors, or stock tickers, where new data keeps arriving every second.

### Data Integration Pipelines
Integration pipelines focus on combining data from multiple systems into a single, unified format. For example, merging customer data from a CRM with transaction data from an e-commerce platform creates a complete picture for analytics or machine learning.

## ETL Pipeline

An **ETL pipeline** (Extract, Transform, Load) is a specific type of data pipeline designed to move and prepare structured data for analysis. These pipelines are the backbone of many analytics systems, ensuring that data flows cleanly from raw sources to organized storage.

### How ETL Pipelines Work
1. **Extract** ‚Äì Data is pulled from one or more sources such as databases, APIs, logs, or flat files.  
2. **Transform** ‚Äì The extracted data is cleaned, reshaped, aggregated, or enriched in a staging area. This ensures the data is consistent and ready for analysis.  
3. **Load** ‚Äì The processed data is moved into its final destination, such as a **data warehouse**, **data lake**, or **analytics platform**.  

This process is especially common in organizations that rely on large volumes of structured data for **reporting, dashboards, and business intelligence**.

### How ETL Pipelines Differ from General Data Pipelines

Every ETL pipeline is a data pipeline, but not every data pipeline follows the ETL structure. Some pipelines may move raw data directly from one location to another (Extract ‚Üí Load) without any transformation, while others follow an ELT approach (Extract, Load, Transform), where transformations occur after loading‚Äîusually inside a warehouse or lake. 

ETL pipelines are typically used when structured data needs to be cleaned and standardized before storage, whereas broader data pipelines may handle real-time streams, machine learning models, or automated event triggers.

### Example

Imagine a company collecting sales data from multiple regional databases. An ETL pipeline would pull all sales records (Extract), clean and standardize currency formats (Transform), and store them in a central data warehouse (Load) for unified reporting.

In short, ETL pipelines focus on **structured data preparation**, while general data pipelines can power many other workflows, from streaming to predictive modeling.

## Pipeline Tools in Python

Python provides several powerful tools for building pipelines:

- **Luigi** ‚Äì workflow management and batch pipeline scheduling.  
- **Airflow** ‚Äì orchestrates complex workflows with dependencies.  
- **Metaflow** ‚Äì simplifies data science pipelines and ML workflow management.  
- **Dagster** ‚Äì modern framework for data orchestrations and testing pipelines.  

In this lesson, we will focus on **Prefect**, a Python workflow orchestration tool that provides a gentle learning curve and allows us to build and run pipelines locally. We'll explain Prefect in detail in the next topic.  

### Further Learning  

Watch this video for an overview of data pipeline basics: https://www.youtube.com/watch?v=DHO2PuR3jrs 

## 2. Prefect

Now that we understand why pipelines are useful, let's look at a tool that helps us build them: **Prefect**.  

Prefect is an open-source workflow orchestration tool that helps you define, run, and monitor your data pipelines. It‚Äôs designed to make workflows more **robust** (automatic retries, error handling) and more **observable** (logs, states, dashboards).

Instead of writing plain Python scripts that can silently fail or become difficult to manage, Prefect gives you structure and visibility. You can break your work into tasks, connect them in flows, and then track everything in real-time through the Prefect UI.

üí° **When to use Prefect:**
- Automating a daily data pipeline (e.g., load ‚Üí clean ‚Üí analyze ‚Üí report).
- Running machine learning training jobs with monitoring.
- Orchestrating ETL workflows across multiple systems.

**üõ†Ô∏è Installation**

Before we start writing any Prefect code, install it on your system (along with supporting libraries we‚Äôll use later)

```bash
pip install prefect pandas matplotlib scipy
```
Once installation completes, you can verify it by running:

```bash
prefect version
```
You should see the installed version number, confirming everything is ready to go.

üëâ At the heart of Prefect are two simple but powerful building blocks - tasks and flows. We‚Äôll explore each of them in the next section.

### 2.1 Core Concepts in Prefect

At its core, Prefect is built around two simple yet powerful ideas: tasks and flows. Together, they form the foundation of every Prefect pipeline.

1. **`@task()` decorator**  

A task is the smallest unit of work in a pipeline - for instance, loading data, cleaning a dataset, or sending a notification. You define a task in Python using the `@task()` decorator. Prefect automatically tracks its state (such as running, successful, or failed), logs its output, and can retry the task if it encounters an error.

üîπ Ideally, each task should be simple and focused - since tasks are the atomic unit of work in Prefect flows.

2. **`@flow()` decorator**  
   
A flow represents the higher-level workflow logic that connects multiple tasks together. You create one using the `@flow()` decorator. A flow orchestrates how and when tasks run, manages dependencies, and serves as the single entry point for running the entire pipeline.

üìñ For more details, you can also check out the [official Prefect docs on flows](https://docs.prefect.io/v3/concepts/flows).

### Simple Example

Let's see a super simple example to get the idea. In your IDE, create a file called `hello_prefect.py` and run from the command line:

```python
from prefect import task, flow

# Define a task
@task
def say_hello(name):
    print(f"Hello, {name}!")

# Define a flow that uses the task
@flow
def output():
    say_hello("Code The Dream")
    say_hello("Students")

# To run the flow
# This condition checks if the current script is the one being executed directly.
if __name__ == "__main__":  
    output()
```
üî∏ Each function decorated with `@task` becomes a distinct step in your workflow. The `@flow` function orchestrates those steps. Running `output()` executes the whole workflow. 

**What Happens When You Run It?**

You may notice that when you run the code, you don‚Äôt just see:

```bash
Hello, Code The Dream!
Hello, Students!
```
Instead, you‚Äôll also see extra startup messages in your terminal - something like this:

![Task&flow](resources/task&flow.png)

**Here‚Äôs what‚Äôs happening:**

When you decorate a function with `@flow`, Prefect orchestrates the workflow rather than just running it like plain Python.

To do this, Prefect spins up a small temporary local server in the background to track the execution of your flow.

That‚Äôs why you see lines like ‚ÄúStarting temporary server on http://127.0.0.1:8597‚Äù

Prefect is not just a typical library with functions you run - it‚Äôs a framework that manages and tracks how your code is executed.

By handing control to Prefect through `@task` and `@flow`, you gain features like Centralized logging, automatic retries on failure, viewing run history in a UI, tracking task states. 

üëâ Later in this lesson, we‚Äôll explore the Orion UI to see how Prefect logs, retries, and statuses appear visually - but for now, just know that Prefect is setting the stage for all that automatically.

### 2.2 How Prefect enhances your workflows

Once you've built a basic pipeline using @task and @flow, Prefect gives you powerful tools to make your workflows more **reliable**, **efficient**, and **production-ready**. In this section, we‚Äôll focus on three essential features: 

**1. Logging**

Instead of relying on `print()`, Prefect provides structured logging through `get_run_logger()`. These logs are automatically captured and shown in the Orion UI, making debugging much easier.

```python
from prefect import task, flow
from prefect.logging import get_run_logger

@task
def greet(name):
    logger = get_run_logger()
    logger.info(f"Hello, {name}!")

@flow
def log_flow():
    greet("Code the Dream")

if __name__ == "__main__":
    log_flow()

```
Console Output:

![Logging](resources/logging.png)

üëâ This is how the logs look in your console output after using `get_run_logger()`.

The same logs also appear in Orion, where they‚Äôre stored for later runs.

### Why Structured Logging Matters

Unlike `print()`, Prefect‚Äôs logger supports log levels like **INFO**, **WARNING**, and **ERROR**.

- **INFO** ‚Äì for general updates about what your flow is doing.  
- **WARNING** ‚Äì for potential issues that don‚Äôt stop execution.  
- **ERROR** ‚Äì for critical problems that cause a task or flow to fail.  

These levels help you filter and search logs efficiently, especially in production environments.

‚ú® **That‚Äôs why logging is a best practice** - logs are timestamped, structured, searchable, and automatically saved across runs.

---

### ‚ö†Ô∏è Heads Up

When running Prefect locally, you might see a message like:

> `EventsWorker - Still processing items...`

This simply means Prefect‚Äôs server is shutting down before all internal logs are fully written.  
It‚Äôs **normal** and doesn‚Äôt affect your results.

If you‚Äôd like to remove the warning, you can add this line at the end of your script:

```python
import time
time.sleep(0.5)
```

**2. Retries and Failure Handling**

Suppose a network call fails intermittently. Prefect allows you to declare retries.

Here's a very simple example showing retries in action with a flaky function using Prefect:

```python
from prefect import flow, task
from random import random
import time

# Define a flaky task
@task(retries=3, retry_delay_seconds=2)
def flaky_task():
    print("Trying to run task...")
    if random() < 0.7:  # 70% chance to fail
        raise ValueError("Task failed! Retrying...")
    print("Task succeeded!")

# Define a flow
@flow
def retry_demo_flow():
    flaky_task()

if __name__ == "__main__":
    retry_demo_flow()
```

 The task `flaky_task` has a 70% chance to fail each run. Prefect automatically retries it up to 3 times, waiting 2 seconds between attempts. If it eventually succeeds within 3 retries, the flow continues normally. If it still fails after 3 retries, the flow marks the task as failed.

**Sample Output (your console or Orion logs):**

```bash
Trying to run task...
Task failed! Retrying...
Trying to run task...
Task failed! Retrying...
Trying to run task...
Task succeeded!

```

This makes your pipeline robust without manual try/except blocks.

**3. Monitoring and Visualization with Orion UI**

Prefect provides a web-based UI - the **Prefect Server UI** (often called **Orion**) - that lets you visualize and monitor your workflows in real time. In Orion, you can see your pipeline‚Äôs execution flow, review detailed task logs and states (including retries), track scheduled runs, and even debug failed steps interactively, all from one convenient dashboard.

*To launch Orion:*

**Step 1:** Start Orion
```bash
prefect server start
```
This command launches the Orion server, which you can access via your web browser at:

```
http://localhost:4200
```

```text
Note:
To launch the Prefect UI (version-dependent).
- For many Prefect 2.x installs run: `prefect orion start`.
- For some later releases the equivalent command is: `prefect server start`.
- If unsure, run `prefect --help` (or check `pip show prefect` / your installed version) and follow the CLI shown by your version.
```

**Step 3: Run Your Flow**

Let‚Äôs use the same example from earlier:

```python
from prefect import flow, task
from prefect.logging import get_run_logger

@task
def greet(name: str):
    logger = get_run_logger()
    logger.info(f"Hello, {name}!")

@flow
def log_flow():
    greet("Code the Dream")

if __name__ == "__main__":
    log_flow()
```
**Step 4: Explore in Orion**

After running your flow, open the **Orion UI** in your browser. The **Dashboard** displays all your active and completed flow runs, giving you a quick overview of what‚Äôs happening. Clicking on a specific flow run opens detailed information about its individual tasks and logs.

Each log entry is structured with timestamps and log levels such as **INFO**, **WARNING**, and **ERROR**, and all logs are stored for future review.

![Orion](resources/orion.png)

As you can see, the Orion UI provides a clear overview of your flow runs, their statuses, and detailed logs - all in one place. This makes it much easier to monitor and debug your workflows compared to relying on terminal output alone.

**Why This Matters**

Prefect automatically tracks your pipeline runs. The Orion UI gives you:

- Real-time monitoring of flows and tasks
- Easy log search & filtering
- Retry history and error tracking

‚ú® That‚Äôs why Prefect encourages logging and visualization in Orion - it turns simple Python scripts into fully observable workflows.

### Advanced Features of Prefect

Prefect isn‚Äôt just about running tasks and viewing logs - it also comes with powerful features to handle real-world workflows. We won‚Äôt dive into these right away, but here‚Äôs a preview of what‚Äôs ahead:

1. Caching: Skip re-running expensive tasks if their inputs haven‚Äôt changed. This makes your workflows faster and more efficient.

2. Parallelism: Run tasks concurrently to speed up data processing and reduce bottlenecks.

3. Scheduling: Automate your flows to run on a set interval or a cron-like schedule, so they can operate hands-free in production.

üìå We‚Äôll explore these in later weeks, once we‚Äôve built a strong foundation with tasks, flows, and the Orion UI.

üì∫ Want to see this in action?
In this video, you‚Äôll see how Prefect not only schedules a Python script but also solves common workflow challenges. 

https://www.youtube.com/watch?v=Kt8GAZRpTcE

### Why Prefect and Not Just Python Functions?  

You might have wondered earlier: *‚ÄúCouldn‚Äôt we just write Python functions and call them in order?‚Äù*  

The short answer is **yes** - you can build a simple pipeline by chaining plain Python functions.  
But the moment you need reliability, monitoring, retries, scheduling, or scaling, plain functions quickly become fragile and hard to manage.  

That‚Äôs where **Prefect** makes the difference. It takes ordinary Python code and turns it into **production-ready workflows** - with features like retries, logging, monitoring, and scheduling built in, so you don‚Äôt have to reinvent the wheel.  

Here‚Äôs the side-by-side view:  

| **Feature**               | **Plain Python**        |  **With Prefect**                    |
|---------------------------|-------------------------|--------------------------------------|
| Retry failed steps        | Manual try/except       | Declarative retries (`retries=3`)    |
| Cache expensive results   | Manual caching          | Built-in caching                     |
| Parallel execution        | Manual threading        | Easy parallel mapping                |
| Monitoring & debugging    | Print statements & logs | UI with history & logs *(Orion)*     |
| Resilience to failures    | Manual error handling   | Automatic retries & recovery         |
| Scheduling workflows      | Manual script runs      | Built-in scheduling & orchestration  |

üëâ In short, Prefect handles the *operational complexity* so you can focus on writing clear analysis logic. 

## 3. Building Your First Prefect Pipeline

Now that we‚Äôve explored Prefect‚Äôs core ideas and enhancements, let‚Äôs bring everything together and build a tiny, end-to-end data analysis pipeline from scratch.

Imagine we‚Äôre analyzing exam scores from two student groups - say, Class A and Class B - to see if there‚Äôs a meaningful difference in their average performance.
We‚Äôll go through the typical steps of a small but realistic data workflow: loading, cleaning, analyzing, visualizing, and reporting results. Along the way, you‚Äôll see how Prefect makes each part more structured, observable, and reusable.

Here‚Äôs what we‚Äôll do:

- Load exam scores for classes A and B.
- Clean and prepare the data (with retries for reliability)
- Perform a statistical test (t-test)
- Visualize the results
- Wrap everything in Prefect tasks and flows with logging

By the end, you‚Äôll have a clear picture of how a simple Python script can evolve into a well-orchestrated Prefect pipeline.

### Step 0: Setup

Before we start coding, let‚Äôs confirm that everything is installed correctly.
This lesson assumes you‚Äôve already installed Prefect and the supporting libraries (as covered in Section 2 ‚Üí üõ†Ô∏è Installation).

Run the following commands in your terminal or notebook cell to verify your setup:

```bash
prefect --version
python -c "import prefect; print(prefect.__version__)"
python -c "import pandas as pd; print(pd.__version__)"
```

If any command fails, revisit the installation steps in Section 2 and ensure your terminal or editor is using the correct Python environment.

‚úÖ Once you see version numbers for both Prefect and Pandas, you‚Äôre ready to move on to the next step.

### Step 1: Imports & Data Loader `(load_data)` with logging

We import necessary libraries:
- `pandas` ‚Üí handle data as tables (`DataFrame`).  
- `prefect` ‚Üí build workflows with tasks/flows.  
- `matplotlib.pyplot` ‚Üí visualize data.  
- `scipy.stats` ‚Üí run statistical tests. 

We also use Prefect‚Äôs logger so messages appear in Orion with timestamps and log levels.

```python
from prefect import task, flow
from prefect.logging import get_run_logger 
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

@task
def load_data() -> pd.DataFrame:
    logger = get_run_logger()
    """
    Create a tiny, readable dataset:
      - Class: which class the student is in ('A' or 'B')
      - Score: exam score (numeric)
    """
    data = {
        "Class": ["A","A","A","A","A", "B","B","B","B","B"],
        "Score": [65, 70, 68, 72, 66,    78, 82, 80, 79, 81]
    }
    df = pd.DataFrame(data)
    logger.info("Exam scores loaded successfully")
    return df

```
üîé Utility:

Instead of using plain print statements, Prefect‚Äôs logs are automatically searchable, timestamped, and stored in the UI, making them easy to review. If your pipeline runs daily, you can quickly filter logs for today‚Äôs runs without sifting through raw console output.

#### üìåFunction Reference:

- pd.DataFrame(data) ‚Üí creates a table (DataFrame) from a dictionary.
- `logger.info("...")` ‚Üí adds timestamped logs in Prefect.
- Task returns a DataFrame for downstream tasks.

üí° Shortcut to repeat labels:

```python
["A"]*5 + ["B"]*5
# ‚Üí  ["A","A","A","A","A", "B","B","B","B","B"]
```
üëâ Next step: Once data is loaded, we need to clean it for reliability.

### Step 2: Clean the Data `(clean_data)` with Retries

Sometimes data is messy or missing. Prefect‚Äôs retries make the pipeline fault-tolerant.

```python
@task(retries=3, retry_delay_seconds=3)
def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    logger = get_run_logger()
    """
    Basic cleaning:
      1) Ensure Score is numeric.
      2) Drop missing values.
    (This is minimal on purpose for clarity.)
    """
    df = df.copy()
    df["Score"] = pd.to_numeric(df["Score"], errors="coerce")
    df = df.dropna(subset=["Score", "Class"])
    logger.info(" Data cleaned")
    return df

```
üîé Utility:

- If the first attempt fails, Prefect waits 3 seconds and tries again (up to 3 retries).
- No manual restart needed ‚Üí pipelines are more resilient.

#### üìåFunction Reference:
- df.copy() ‚Üí makes a safe copy so we don‚Äôt overwrite the original.
- pd.to_numeric(..., errors="coerce") ‚Üí converts values to numbers (invalid ‚Üí NaN).
- df.dropna(...) ‚Üí removes rows with missing values

üëâ Once the data is clean, the next step is to explore and visualize it so we can understand class-wise performance.

### Step 3: Describe & Plot (describe_and_plot)

After cleaning, let‚Äôs summarize the dataset and visualize the score distribution.

```python
@task
def describe_and_plot(df: pd.DataFrame) -> None:
    logger = get_run_logger()
    """
    - Log summary stats per class.
    - Make a boxplot comparing Class X vs Class Y scores.
    - Save the plot.
    """
    # Generate descriptive statistics for each class
    summary = df.groupby("Class")["Score"].describe()
    logger.info("Summary statistics per class:\n%s", summary)

    # Plot
    ax = df.boxplot(by="Class", column="Score")
    plt.title("Exam Scores by Class")
    plt.suptitle("")  # removes automatic 'Score by Class' super-title
    plt.xlabel("Class")
    plt.ylabel("Score")

    # Save the plot
    plt.savefig("scores_boxplot.png")
    logger.info('Plot saved to "scores_boxplot.png"')
    plt.close()

```
When this task runs, it logs something like:

```
       count   mean   std   min   25%   50%   75%   max
Class
A        5.0  68.2   2.59  65.0  66.0  68.0  70.0  72.0
B        5.0  80.0   1.87  78.0  79.0  80.0  81.0  82.0
```
**Explanation**

Here, `.describe()` automatically gives us a mini summary for each group (Class A and Class B):

- **count** tells us how many scores are in the group.
- **mean** is the average score.
- **std** (standard deviation) shows how spread out the scores are.
- **min** and **max** are the lowest and highest scores.
- **25%, 50%, 75%** are the quartiles - with 50% being the median.

From the output, we can see that **Class B has higher average scores (80 vs 68.2)**, and both groups are fairly consistent since the standard deviation is small.

![Output_pipeline](resources/Output_pipeline.png)

#### üìåFunction Reference: 

- df.groupby("Class") ‚Üí splits data into Class A and Class B. 
- .describe() ‚Üí gives stats: count, mean, std, min, max, quartiles. 
- df.boxplot(by="Class", column="Score") ‚Üí makes side-by-side boxplots.
- plt.savefig("file.png") ‚Üí saves plot as an image.

üëâ With this overview, we now have both numerical summaries and visual patterns, making it easier to move into statistical testing in the next step.

### Step 4: Run a t-test (run_ttest)

Once we‚Äôve explored the data, the next step is to formally test whether the difference between Class A and Class B is statistically significant. For this, we use a t-test.

```python
@task
def run_ttest(df: pd.DataFrame) -> tuple[float, float]:
    """
    Independent samples t-test:
      - Compares average Score between Class A and Class B.
      - Returns (t_statistic, p_value).
    """
    logger = get_run_logger()

    a = df[df["Class"] == "A"]["Score"]
    b = df[df["Class"] == "B"]["Score"]

    # Welch‚Äôs t-test is more robust when groups have unequal variance
    t_stat, p_val = ttest_ind(a, b, equal_var=False)

    logger.info("T-test result: t=%.2f, p=%.4f", t_stat, p_val)
    return t_stat, p_val
```

#### üìå Function Reference:

- df[df["Class"] == "A"]["Score"] ‚Üí filter rows where Class = A, get scores.
- ttest_ind(a, b, equal_var=False) ‚Üí compares mean of group A vs group B.
- t-statistic = size of the difference relative to variation.
- Null hypothesis (H0): both classes have the same average score.
- p-value: probability we‚Äôd see a difference this large if H0 were true.

**T-test output**
```bash
T-test result: t = -8.07, p = 0.0002
```

**Interpreting Results:**

**1. t = -8.07**

The t-statistic (t = -8.07) measures how many standard errors the difference between the group means is away from zero. The negative sign indicates that the mean of Class A is lower than that of Class B, and the large magnitude (8.07) shows a strong difference between the groups.

**2. p = 0.0002**

The p-value (p = 0.0002) tells us the probability of observing such a difference if the two groups were actually the same. Because this value is much smaller than 0.05, the result is statistically significant, meaning the difference in scores is unlikely to be due to chance.

‚úÖ With the t-test done, we now have statistical evidence. But numbers alone aren‚Äôt enough -  let‚Äôs translate them into clear conclusions in the next step.

### Step 5: Report Results (report_results)

Finally, we summarize the statistical test by showing the group means and stating whether the difference is significant.

```python
@task
def report_results(ttest_result: tuple[float, float], df: pd.DataFrame) -> None:
    """
    Report the results of the t-test with group means.
    """
    logger = get_run_logger()

    t_stat, p_val = ttest_result
    mean_a = df[df["Class"] == "A"]["Score"].mean()
    mean_b = df[df["Class"] == "B"]["Score"].mean()

    logger.info("Class A mean: %.1f, Class B mean: %.1f", mean_a, mean_b)

    if p_val < 0.05:
        logger.info("Conclusion: The difference is statistically significant (p < 0.05).")
    else:
        logger.info("Conclusion: No statistically significant difference (p ‚â• 0.05).")
```

**Sample Output:**

```bash
Conclusion: The difference is statistically significant (p < 0.05)
```

üìå Function Reference:
- .mean() ‚Üí calculates average of values.
- if p_val < 0.05: ‚Üí 5% threshold is a common cutoff for significance.

With this final reporting step, your pipeline now summarizes group means and explains whether the difference is statistically meaningful.

### Step 6: Orchestrate Everything (@flow)

We bring it all together using the `@flow` decorator:

```python
@flow
def analysis_pipeline():
    """
    The *recipe* that runs all steps in order.
    """
    df = load_data()
    clean_df = clean_data(df)
    describe_and_plot(clean_df)
    result = run_ttest(clean_df)
    report_results(result, clean_df)

if __name__ == "__main__":
    analysis_pipeline()
```
Now you have a full mini pipeline - from data loading to cleaning, visualization, statistical testing, and reporting - all orchestrated with Prefect in a single flow.

- The **@flow** decorator turns `analysis_pipeline` into a complete workflow.
- When called, it executes all steps in sequence, automatically passing data between tasks.

‚ú® This gives you a clear, reproducible, and automated workflow that‚Äôs easy to run, track, and extend as your project grows.

üìù In your homework, we‚Äôll dive deeper into the Orion UI, where you‚Äôll run this pipeline, explore logs, observe retries, and monitor task states in a dashboard.

## 4. Wrap-up

In this lesson, you‚Äôve learned how pipelines automate complex workflows, making your data analysis **reproducible**, **modular**, and **scalable**. Prefect enhances this process by providing decorators for tasks and flows, built-in centralized **logging**, **retries**, caching, and seamless **monitoring** through the *Orion* dashboard. By building your pipeline from clear, modular steps, you can run the entire analysis with one command, handle failures automatically, and monitor your workflow in real time, all while keeping your code maintainable.

‚úçÔ∏è For your upcoming assignment, you‚Äôll build your own data pipeline following this structure. Define each analysis step as a Prefect task, orchestrate them in a flow, and consider how you can use retries and logging to make your pipeline robust and observable. Think about how the Orion dashboard can help you monitor and debug your workflow efficiently as you run it on different datasets.

**üëè Well done!**
You just walked through your very first Prefect pipeline. üéâKeep this momentum for your assignment. 